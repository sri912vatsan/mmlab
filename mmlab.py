# -*- coding: utf-8 -*-
"""MM Lab CA1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14V9s9IfdxPdKYW4iwAHoTsIQZeltwn2e

## **Problem Sheet 1**

### **Q1**
Genrate (i) 500, (ii) 1000, (iii) 10,000 and (iv) 1,00,000 random numbers for the following distributions
Draw the histogram and frequency polygon
(i) Uniform distribution (ii) exponential distribution (iii) Weibull distribution (iv) triangular
distribution
"""

import numpy as np
import matplotlib.pyplot as plt

n = 100000
y = np.random.uniform(0, 1, size = n)
plt.hist(y, label = 'Histogram', bins = 50)
plt.plot(np.histogram(y, bins = 50)[1][:-1], np.histogram(y, bins = 50)[0], label = 'Frequency Polygon')
plt.legend()
plt.show()

y = np.random.exponential(1, n)
plt.hist(y, label = 'Histogram', bins = 50)
plt.plot(np.histogram(y, bins = 50)[1][:-1], np.histogram(y, bins = 50)[0], label = 'Frequency Polygon')
plt.legend()
plt.show()

y = np.random.weibull(2, n)
plt.hist(y, label = 'Histogram', bins = 50)
plt.plot(np.histogram(y, bins = 50)[1][:-1], np.histogram(y, bins = 50)[0], label = 'Frequency Polygon')
plt.legend()
plt.show()

y = np.random.triangular(0, 0.5, 1, n)
plt.hist(y, label = 'Histogram', bins = 50)
plt.plot(np.histogram(y, bins = 50)[1][:-1], np.histogram(y, bins = 50)[0], label = 'Frequency Polygon')
plt.legend()
plt.show()

"""### **Q2**
Estimating π
"""

import numpy as np
import matplotlib.pyplot as plt

n = 100000

inCircle = 0
inCircleX = []
inCircleY = []
X = []
Y = []

for i in range(n):
    x = np.random.uniform(-1, 1)
    y = np.random.uniform(-1, 1)
    if x ** 2 + y ** 2 <= 1:
        inCircle += 1
        inCircleX.append(x)
        inCircleY.append(y)
    else:
        X.append(x)
        Y.append(y)

plt.figure(figsize = (10, 10))
plt.scatter(inCircleX, inCircleY, color = 'r', label = 'Inside Circle', s = 0.2)
plt.scatter(X, Y, color = 'navy', label = 'Outside Circle', s = 0.2)
plt.legend()
plt.title('Estimated Value of Pi = {}'.format(4 * inCircle / n))
plt.suptitle('Estimating Pi')
plt.tight_layout()
plt.show()

"""### **Q3**
Evaluate the integration by Monte carlo simulation
using N=1000, 1,00,000 sketch the graph of the function also.
"""

import numpy as np
import matplotlib.pyplot as plt

def f(x):
    return np.sin(np.pi * np.cos(3 * x)) ** 2 * np.cos(x) ** 2

def monteCarloIntegration(f, a, b, n):
    x = np.random.uniform(a, b, n)
    y = f(x)
    return np.mean(y) * (b - a)

a = 0
b = np.pi
N = [10000, 100000, 1000000]

for n in N:
    print(monteCarloIntegration(f, a, b, n))

x = np.linspace(a, b, 1000000)
y = f(x)
plt.plot(x, y, label = 'f(x)')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title("Plot of the Function")
plt.show()

"""### **Q4**
Simulate (with 1000 samples) the queuing system FIFO and calculate its parameters (i) average
waiting time in the queue (ii)average length of the queue (iii) average waiting time in the system.
(a)Poisson arrival with mean of 3 sec and exponential service time with mean 4sec
(b) uniform arrival with mean 3 sec and exponenl service time with mean 4sec.
"""

import numpy as np

def poisson(n, mean = 3):
    return np.random.poisson(mean, n)

def uniform(n, mean = 3):
    return np.random.uniform(0, 2 * mean, n)

def exponential(n, mean = 4):
    return np.random.exponential(mean, n)

def simulateQueue(arrivalFunc, serviceFunc, n):
    arrivalTimes = arrivalFunc(n)
    serviceTimes = serviceFunc(n)
    queueWaitingTimes = []
    systemWaitingTimes = []
    lengthOfQueue = []

    arrivalTime = arrivalTimes[0]
    departureTime = arrivalTime + serviceTimes[0]

    queueWaitingTimes.append(0)
    systemWaitingTimes.append(departureTime)
    queue = []
    lengthOfQueue.append(len(queue))

    for i in range(1, n):
        arrivalTime += arrivalTimes[i]

        while len(queue) > 0 and queue[0] <= arrivalTime:
            queue.pop(0)

        if arrivalTime < departureTime:
            queueTime = departureTime - arrivalTime
            queue.append(departureTime)
        else:
            queueTime = 0
            queue = []
        systemTime = queueTime + serviceTimes[i]
        departureTime += serviceTimes[i]

        queueWaitingTimes.append(queueTime)
        systemWaitingTimes.append(systemTime)
        lengthOfQueue.append(len(queue))

    print("Average Queue Waiting Time:", np.mean(queueWaitingTimes))
    print("Average System Waiting Time:", np.mean(systemWaitingTimes))
    print("Average Length of Queue:", np.mean(lengthOfQueue))

simulateQueue(poisson, exponential, 1000)
print('==============================================')
simulateQueue(uniform, exponential, 1000)

"""## **Problem Sheet 2**

### **Q1**
Obtain the  temperature of   Coimbatore for the months in 2023

   (i) Interpolate using generation of  1000 random numbers, sketch the graph and validate the model by   
       comparing with original values using testing of hypothesis

   (ii) Using the interpolating polynomial obtained in (i) Find the mean temperatures for the months of 2024
       and compare with original results.
"""

import pandas as pd
import matplotlib.pyplot as plt
import random
import numpy as np
from scipy.interpolate import interp1d
from scipy import stats

'''
f_oneway is a function from the scipy.stats module in Python.
It performs a one-way ANOVA (Analysis of Variance) test.
This test is used to determine if there are statistically significant
differences between the means of two or more independent groups.
'''
def compare(x, y):
    f, p = stats.f_oneway(x, y)
    print(f"F Value: {f:.2f}")
    print(f"p Value: {p:.2f}")
    alpha = 0.05
    if p < alpha:
        print("There is significant difference")
    else:
        print("There is no significant difference")

def newtonInterpolation(x, y, z):
    n = len(x)
    ddt = [[0 for i in range(n)] for j in range(n)]
    for i in range(n):
        ddt[i][0] = y[i]

    for i in range(1, n):
        for j in range(n - i):
            ddt[j][i] = ddt[j + 1][i - 1] - ddt[j][i - 1]

    res = ddt[0][0]
    u = (z - x[0]) / (x[1] - x[0])
    for i in range(1, n):
        temp = 1
        for j in range(i):
            temp *= (u - j) / (j + 1)
        temp *= ddt[0][i]
        res += temp

    return res

def splineInterpolation(x, y, z):
    splineFun = interp1d(x, y, kind = 'cubic')
    return splineFun(z)

def lagrangePolynomial(x, y, z):
    res = 0
    for i in range(len(x)):
        numerator = 1
        denominator = 1
        for j in range(len(x)):
            if i != j:
                numerator *= (z - x[j])
                denominator *= (x[i] - x[j])
        res += (numerator / denominator) * y[i]
    return res

with open("/content/drive/MyDrive/SEM-9/MM Lab/data.xls") as file:
    df = pd.read_csv(file)

df = df.loc[df['Year'] == 2020]
df = df.reset_index(drop = True)
monthTemp = df.groupby(df['Month'])['AvgTemperature'].mean().tolist()
month = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
plt.scatter(month, monthTemp, label = 'Actual', c = 'r')

Z = np.arange(1, 12, 0.1).tolist()

plt.plot([1 + ((x - 1) / (364)) * 11 for x in range(1, 366)], [df['AvgTemperature'][i - 1] for i in range(1, 366)], label = 'Actual', linewidth = 0.7)
plt.plot(Z, [newtonInterpolation(month, monthTemp, z) for z in Z], label = 'Newton Interpolation')
plt.plot(Z, [splineInterpolation(month, monthTemp, z) for z in Z], label = 'Spline Interpolation')
plt.plot(Z, [lagrangePolynomial(month, monthTemp, z) for z in Z], label = 'Lagrange Polynomial', linestyle = '--')
plt.legend()
plt.show()

randomDays365 = random.sample(range(1, 365), 50)
randomDays365.sort()
randomDays12 = [1 + ((x - 1) / (364)) * 11 for x in randomDays365]
actualTemps = [df['AvgTemperature'][i - 1] for i in randomDays365]

compare(actualTemps, [newtonInterpolation(month, monthTemp, z) for z in randomDays12])
compare(actualTemps, [splineInterpolation(month, monthTemp, z) for z in randomDays12])
compare(actualTemps, [lagrangePolynomial(month, monthTemp, z) for z in randomDays12])

"""## **Problem Sheet 3**

### **Q1**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score
from scipy.stats import shapiro

with open("/content/drive/MyDrive/SEM-9/MM Lab/data.xls") as file:
    df = pd.read_csv(file)

Y = df['AvgTemperature'].tolist()
X = [i for i in range(len(Y))]
xTrain = X[:7000]
yTrain = Y[:7000]
xTest = X[7000:]
yTest = Y[7000:]

model1 = np.poly1d(np.polyfit(xTrain, yTrain, 1))
yPred1 = model1(xTest)
model2 = np.poly1d(np.polyfit(xTrain, yTrain, 2))
yPred2 = model2(xTest)
model3 = np.poly1d(np.polyfit(xTrain, yTrain, 3))
yPred3 = model3(xTest)

fig, axes = plt.subplots(3, 1, figsize = (20, 5))

axes[0].plot(xTest, yTest, label = 'Actual', linewidth = 0.7)
axes[0].plot(xTest, yPred1, label = 'Predicted')
axes[0].legend()
axes[0].set_title('Linear Regression')

axes[1].plot(xTest, yTest, label = 'Actual', linewidth = 0.7)
axes[1].plot(xTest, yPred2, label = 'Predicted')
axes[1].legend()
axes[1].set_title('Quadratic Regression')

axes[2].plot(xTest, yTest, label = 'Actual', linewidth = 0.7)
axes[2].plot(xTest, yPred3, label = 'Predicted')
axes[2].legend()
axes[2].set_title('Cubic Regression')

plt.legend()
plt.tight_layout()
plt.show()

print("Coefficient of Determination:", r2_score(yTest, yPred1))
print("Coefficient of Determination:", r2_score(yTest, yPred2))
print("Coefficient of Determination:", r2_score(yTest, yPred3))

residuals1 = yTest - yPred1
residuals2 = yTest - yPred2
residuals3 = yTest - yPred3

print("Mean =", np.mean(residuals1))
print("Mean =", np.mean(residuals2))
print("Mean =", np.mean(residuals3))

stat, p = shapiro(residuals1)
if p < 0.05:
    print("Not Normally Distributed")
else:
    print("Maybe Normally Distributed")

stat, p = shapiro(residuals2)
if p < 0.05:
    print("Not Normally Distributed")
else:
    print("Maybe Normally Distributed")

stat, p = shapiro(residuals3)
if p < 0.05:
    print("Not Normally Distributed")
else:
    print("Maybe Normally Distributed")

"""### **Q3**
Using the equations obtained in the previous problem generate the models M i = R j + E k (where R j
denotes the regression models obtain in (i) , (ii) and (iii) and E k denotes the model obtained in (vi) (a) ,
(b) and (c). Validate each model with original values obtained for 2024 ( 30/6/2024) (iv) perform ANOVA
test for the hypothesis there is no significant difference between the models.
"""

import pandas as pd
import numpy as np
from datetime import datetime

def dayOfYear(date):
    date = date.strftime('%Y-%m-%d')
    date = datetime.strptime(date, '%Y-%m-%d')
    return date.timetuple().tm_yday

df = pd.read_csv('/content/drive/MyDrive/SEM-9/MM Lab/SBI2023csv.csv')
df['Date'] = pd.to_datetime(df['Date'])
df = df[['Date', 'Open Price', 'Close Price']]
df['Date'] = df['Date'].apply(dayOfYear)
df = df.rename(columns = {'Date': 'Day'})
df = df.sort_values(by = 'Day')
df['RoR'] = (df['Close Price'] - df['Open Price']) / df['Open Price']

X = df['Day'].tolist()
Y = df['RoR'].tolist()
randomValues = random.sample(range(0, len(X)), 150)
xTrain = [X[i] for i in randomValues]
yTrain = [Y[i] for i in randomValues]


model1 = np.poly1d(np.polyfit(xTrain, yTrain, 1))
yPred1 = model1(X)
model2 = np.poly1d(np.polyfit(xTrain, yTrain, 2))
yPred2 = model2(X)
model3 = np.poly1d(np.polyfit(xTrain, yTrain, 3))
yPred3 = model3(X)

residuals1 = Y - yPred1
residuals2 = Y - yPred2
residuals3 = Y - yPred3

mean1 = np.mean(residuals1)
std1 = np.std(residuals1)
mean2 = np.mean(residuals2)
std2 = np.std(residuals2)
mean3 = np.mean(residuals3)
std3 = np.std(residuals3)

fig, axes = plt.subplots(3, 3, figsize = (10, 7))

axes[0, 0].plot(X, model1(Y) + np.random.normal(mean1, std1, len(X)), linewidth = 0.5, label = 'Predicted', c = 'magenta')
axes[0, 1].plot(X, model1(Y) + np.random.normal(mean2, std2, len(X)), linewidth = 0.5, label = 'Predicted', c = 'magenta')
axes[0, 2].plot(X, model1(Y) + np.random.normal(mean3, std3, len(X)), linewidth = 0.5, label = 'Predicted', c = 'magenta')
axes[1, 0].plot(X, model2(Y) + np.random.normal(mean1, std1, len(X)), linewidth = 0.5, label = 'Predicted', c = 'orange')
axes[1, 1].plot(X, model2(Y) + np.random.normal(mean2, std2, len(X)), linewidth = 0.5, label = 'Predicted', c = 'orange')
axes[1, 2].plot(X, model2(Y) + np.random.normal(mean3, std3, len(X)), linewidth = 0.5, label = 'Predicted', c = 'orange')
axes[2, 0].plot(X, model3(Y) + np.random.normal(mean1, std1, len(X)), linewidth = 0.5, label = 'Predicted', c = 'yellow')
axes[2, 1].plot(X, model3(Y) + np.random.normal(mean2, std2, len(X)), linewidth = 0.5, label = 'Predicted', c = 'yellow')
axes[2, 2].plot(X, model3(Y) + np.random.normal(mean3, std3, len(X)), linewidth = 0.5, label = 'Predicted', c = 'yellow')
axes[0, 0].set_title('Linear Model - Linear Error')
axes[0, 1].set_title('Linear Model - Quadratic Error')
axes[0, 2].set_title('Linear Model - Cubic Error')
axes[1, 0].set_title('Quadratic Model - Linear Error')
axes[1, 1].set_title('Quadratic Model - Quadratic Error')
axes[1, 2].set_title('Quadratic Model - Cubic Error')
axes[2, 0].set_title('Cubic Model - Linear Error')
axes[2, 1].set_title('Cubic Model - Quadratic Error')
axes[2, 2].set_title('Cubic Model - Cubic Error')

for i in range(3):
    for j in range(3):
        axes[i, j].plot(X, Y, label = 'Actual', linewidth = 0.5, c = 'navy')
        axes[i, j].axis(xmin = 0, xmax = 350)
        # axes[i, j].legend()

plt.suptitle('Combined Models')
plt.tight_layout()
plt.show()

models = [model1, model2, model3]
means = [mean1, mean2, mean3]
stds = [std1, std2, std3]

alpha = 0.05
for i in range(3):
    for j in range(3):
        print(i, j)
        predY = models[i](X) + np.random.normal(means[j], stds[j], len(X))
        f, p = stats.f_oneway(Y, predY)
        if p < alpha:
            print("There is significant difference")
        else:
            print("There is no significant difference")

"""## **Problem Sheet 4**

### **Q1**
"""

import pandas as pd
from statsmodels.tsa.stattools import acf, pacf, adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima.model import ARIMA

def checkStationary(values):
    result = adfuller(values)
    if result[1] <= 0.5:
        return 1
    else:
        return 0

with open("/content/drive/MyDrive/SEM-9/MM Lab/PS4 Data.csv") as file:
    df = pd.read_csv(file)

if checkStationary(df['Reading']):
    print("Stationary")
else:
    print("Not Stationary")

plot_acf(df['Reading'], lags = 20)
plt.show()
plot_pacf(df['Reading'], lags = 20)
plt.show()

acfValues = acf(df['Reading'], nlags = 20)
pacfValues = pacf(df['Reading'], nlags = 20)

model = ARIMA(df['Reading'], order = (17, 0, 1))
modelFit = model.fit()
print(modelFit.summary())

plt.plot(modelFit.resid)
plt.title('Residuals Plot')
plt.show()

"""### **Q3**
A time series data is fitted through the following linear filter. Obtain the 50,000 values of the time series, obtain  the ACF and PACF for time lag up to k = 24 and discuss its nature using ACF.
Fit the ARMA model for the orders given in problem 2.
(i) yt=10+0.75 yt-1+t    (generate the error  from N(0,10)) (y0=31)
(ii) yt=2+0.75 yt-1+t      (generate the error  from N(0,4)) (y0=42)
(iiI) yt=20+ yt-1+t          (generate the error  from N(0,20)) (y0=30)
"""

import numpy as np
import matplotlib.pyplot
from statsmodels.tsa.stattools import acf, pacf
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf, plot_predict
from statsmodels.tsa.arima.model import ARIMA

list1 = [31]
list2 = [42]
list3 = [30]

for i in range(1, 50000):
    list1.append(10 + 0.75 * list1[i - 1] + np.random.normal(0, 10))
    list2.append(2 + 0.75 * list2[i - 1] + np.random.normal(0, 4))
    list3.append(20 + list3[i - 1] + np.random.normal(0, 20))

fig, axes = plt.subplots(3, 2, figsize = (10, 7))
plot_acf(list1, lags = 24, ax = axes[0, 0])
plot_pacf(list1, lags = 24, ax = axes[0, 1])
plot_acf(list2, lags = 24, ax = axes[1, 0])
plot_pacf(list2, lags = 24, ax = axes[1, 1])
plot_acf(list3, lags = 24, ax = axes[2, 0])
plot_pacf(list3, lags = 24, ax = axes[2, 1])
plt.tight_layout()
plt.show()

model1 = ARIMA(list1, order = (1, 0, 1))
model2 = ARIMA(list2, order = (1, 0, 1))
model3 = ARIMA(list3, order = (1, 0, 1))
modelFit1 = model1.fit()
modelFit2 = model2.fit()
modelFit3 = model3.fit()

fig, axes = plt.subplots(3, 1, figsize = (15, 6))

axes[0].plot(list1[49900:])
axes[0].legend()
plot_predict(modelFit1, start = 49900, end = 50010, ax = axes[0])
axes[1].plot(list2[49900:])
axes[1].legend()
plot_predict(modelFit2, start = 49900, end = 50010, ax = axes[1])
axes[2].plot(list3[49900:])
axes[2].legend()
plot_predict(modelFit3, start = 49900, end = 50010, ax = axes[2])

plt.tight_layout()
plt.show()

"""## **Problem Sheet 5**

### **Q1**
If   Ψ={Yit} be the sequence of  time series  and d Yit,Yjt=1-i,j2 ,where  i,j2
    is the correlation coefficient between Yit  and Yjt, is a metric on  . The resulting space
     is called correlation space

Consider the Niffty50 stock data from 01/01/2014 to  the 31/12/2023. Construct the       distance matrix in the correlation space   

Identify (i)30 (ii) 35 and (iii) 40 dominant Eigen values and corresponding Eigen vectors. Express other stock values as linear  combinations of these  stock values possessing dominant eigenvalues . Compare   with the original values and identify the best training set among (i) , (ii) and (iii) and validate the result.

Obtain the Minimum spanning tree using  (i) correlation metric and (ii)Euclidean metric and for the Niffty 50 stock data and cluster the data using the minimum spanning tree using Prims and Kruskal algorithms for both the metrics.

Obtain the hierarchical clustering using Dendrogram (bottom top approach).  Find the optimal number of clusters using di,j<0.5 and  display the cluster.

Using the optimal clustering data obtained in (iii),  predict the Nifty stock movement using regression models and validate the result.
"""

!pip install yfinance

import yfinance as yf
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

nifty50Tickers = ['RELIANCE.NS', 'HDFCBANK.NS', 'ICICIBANK.NS', 'INFY.NS', 'TCS.NS', 'SHRIRAMFIN.NS', 'ITC.NS', 'KOTAKBANK.NS', 'AXISBANK.NS', 'SBIN.NS', 'LT.NS', 'BAJFINANCE.NS', 'MARUTI.NS', 'BHARTIARTL.NS', 'HINDUNILVR.NS', 'M&M.NS', 'ASIANPAINT.NS', 'TITAN.NS', 'ULTRACEMCO.NS', 'SUNPHARMA.NS', 'TECHM.NS', 'HCLTECH.NS', 'WIPRO.NS', 'NESTLEIND.NS', 'BAJAJFINSV.NS', 'POWERGRID.NS', 'NTPC.NS', 'TATAMOTORS.NS', 'ONGC.NS', 'TATASTEEL.NS', 'JSWSTEEL.NS', 'COALINDIA.NS', 'ADANIPORTS.NS', 'HINDALCO.NS', 'BPCL.NS', 'CIPLA.NS', 'DRREDDY.NS', 'EICHERMOT.NS', 'GRASIM.NS', 'DIVISLAB.NS', 'BRITANNIA.NS', 'UPL.NS', 'HEROMOTOCO.NS', 'SHREECEM.NS', 'INDUSINDBK.NS', 'SBILIFE.NS', 'BAJAJ-AUTO.NS', 'HDFCLIFE.NS', 'ICICIGI.NS', 'APOLLOHOSP.NS']
data = pd.DataFrame()

for ticker in nifty50Tickers:
    tickerData = yf.download(ticker, start = '2014-01-01', end = '2023-12-31', progress = False)
    data[ticker] = tickerData['Close']

data.plot(figsize = (20, 12), linewidth = 0.7)
plt.yscale('log')
plt.title('Log Close Prices of Nifty 50 Stocks(2014 - 2023)')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend(loc = 'center left', fontsize = 'x-small', bbox_to_anchor = (1, 0.5))
plt.show()

plt.figure(figsize = (20, 20))
correlationMatrix = data.corr()
sns.heatmap(correlationMatrix, annot = False, linewidths = 0.5, linecolor = 'black')
plt.show()

import numpy as np
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error

distanceMatrix = 1 - correlationMatrix ** 2
distanceMatrix = distanceMatrix.to_numpy()

pca30 = PCA(n_components = 30)
pca30.fit(distanceMatrix)
eValues30 = pca30.explained_variance_
eVectors30 = pca30.components_
transformed30 = pca30.transform(distanceMatrix)
reconstructed30 = pca30.inverse_transform(transformed30)

pca35 = PCA(n_components = 35)
pca35.fit(distanceMatrix)
eValues35 = pca35.explained_variance_
eVectors35 = pca35.components_
transformed35 = pca35.transform(distanceMatrix)
reconstructed35 = pca35.inverse_transform(transformed35)

pca40 = PCA(n_components = 40)
pca40.fit(distanceMatrix)
eValues40 = pca40.explained_variance_
eVectors40 = pca40.components_
transformed40 = pca40.transform(distanceMatrix)
reconstructed40 = pca40.inverse_transform(transformed40)

mse30 = mean_squared_error(distanceMatrix, reconstructed30)
mse35 = mean_squared_error(distanceMatrix, reconstructed35)
mse40 = mean_squared_error(distanceMatrix, reconstructed40)

print("Reconstruction Error (30 Components): {:.10f}".format(mse30))
print("Reconstruction Error (35 Components): {:.10f}".format(mse35))
print("Reconstruction Error (40 Components): {:.10f}".format(mse40))

import networkx as nx

G = nx.from_numpy_array(distanceMatrix)
T = nx.minimum_spanning_tree(G)
nodeLabels = {i: ticker for i, ticker in enumerate(nifty50Tickers)}

plt.figure(figsize = (15, 15))
nx.draw(T, with_labels = True, font_size = 5, labels = nodeLabels, node_size = 2000, pos = nx.circular_layout(T))
plt.title('Minimum Spanning Tree of Nifty50 Stocks')
plt.show()

from scipy.spatial.distance import cdist

corrDistMatrix = 1 - correlationMatrix
corrDistMatrix = corrDistMatrix.to_numpy()
corrG = nx.from_numpy_array(corrDistMatrix)
corrPrim = nx.minimum_spanning_tree(corrG, algorithm = 'prim')
corrKrus = nx.minimum_spanning_tree(corrG, algorithm = 'kruskal')

dailyReturns = data.pct_change().dropna()
euclideanMatrix = cdist(dailyReturns.T, dailyReturns.T, metric = 'euclidean')
eucG = nx.from_numpy_array(euclideanMatrix)
eucPrim = nx.minimum_spanning_tree(eucG, algorithm = 'prim')
eucKrus = nx.minimum_spanning_tree(eucG, algorithm = 'kruskal')

fig, axes = plt.subplots(2, 2, figsize = (10, 10))

nx.draw(corrPrim, with_labels = True, font_size = 2, labels = nodeLabels, node_size = 250, pos = nx.circular_layout(corrPrim), ax = axes[0, 0])
axes[0, 0].set_title('Correlation Distance - Prim')
nx.draw(corrKrus, with_labels = True, font_size = 2, labels = nodeLabels, node_size = 250, pos = nx.circular_layout(corrKrus), ax = axes[0, 1])
axes[0, 1].set_title('Correlation Distance - Kruskal')
nx.draw(eucPrim, with_labels = True, font_size = 2, labels = nodeLabels, node_size = 250, pos = nx.circular_layout(eucPrim), ax = axes[1, 0])
axes[1, 0].set_title('Euclidean Distance - Prim')
nx.draw(eucKrus, with_labels = True, font_size = 2, labels = nodeLabels, node_size = 250, pos = nx.circular_layout(eucKrus), ax = axes[1, 1])
axes[1, 1].set_title('Euclidean Distance - Kruskal')

plt.tight_layout()
plt.show()

from scipy.cluster.hierarchy import dendrogram, linkage

linked = linkage(distanceMatrix, 'single')

plt.figure(figsize = (10, 7))
dendrogram(linked,
           orientation = 'top',
           distance_sort = 'descending',
           show_leaf_counts = True,
           labels = nifty50Tickers)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Stock')
plt.ylabel('Distance')
plt.axhline(y = 0.5, color = 'r', linestyle = '--')
plt.show()

"""## **Practice**"""

import numpy as np
import matplotlib.pyplot as plt

n = 100000

y1 = np.random.uniform(0, 1, n)
plt.hist(y1, bins = 50)
plt.plot(np.histogram(y1, bins = 50)[1][:-1], np.histogram(y1, bins = 50)[0])
plt.show()

y2 = np.random.exponential(1, n)
plt.hist(y2, bins = 50)
plt.plot(np.histogram(y2, bins = 50)[1][:-1], np.histogram(y2, bins = 50)[0])
plt.show()

y3 = np.random.weibull(2, n)
plt.hist(y3, bins = 50)
plt.plot(np.histogram(y3, bins = 50)[1][:-1], np.histogram(y3, bins = 50)[0])
plt.show()

y4 = np.random.triangular(0, 0.5, 1, n)
plt.hist(y4, bins = 50)
plt.plot(np.histogram(y4, bins = 50)[1][:-1], np.histogram(y4, bins = 50)[0])
plt.show()

import numpy as np
import matplotlib.pyplot as plt

n = 100000
inside = 0
inX = []
inY = []
outX = []
outY = []
for i in range(n):
    x = np.random.uniform(-1, 1)
    y = np.random.uniform(-1, 1)
    if x **2 + y **2 <= 1:
        inside += 1
        inX.append(x)
        inY.append(y)
    else:
        outX.append(x)
        outY.append(y)

plt.figure(figsize = (10, 10))
plt.scatter(inX, inY, c = 'b', s = 0.5)
plt.scatter(outX, outY, c = 'r', s = 0.5)
plt.show()
print(4 * inside / n)

import numpy as np
import matplotlib.pyplot as plt

def f(x):
    return np.sin(np.pi * np.cos(np.cos(3 * x))) ** 2 * np.cos(x) ** 2
    # return np.sin(np.pi * np.cos(3 * x)) ** 2 * np.cos(x) ** 2

def monteCarlo(f, a, b, n):
    x = np.random.uniform(a, b, n)
    y = f(x)
    return np.mean(y) * (b - a)

print(monteCarlo(f, 0, np.pi, 1000000))

x = np.linspace(0, np.pi, 1000000)
y = f(x)
plt.plot(x, y)
plt.show()

import numpy as np

def poissonArrival(n, mean = 3):
    return np.random.poisson(mean, n)

def uniformArrival(n, mean = 3):
    return np.random.uniform(0, 2 * mean, n)

def exponentialService(n, mean = 4):
    return np.random.exponential(mean, n)

def simulation(arrivalFunc, serviceFunc, n):
    arrivalTimes = arrivalFunc(n)
    serviceTimes = serviceFunc(n)

    qWaiting = []
    sWaiting = []
    queue = []
    queueLength = []

    arrivalTime = arrivalTimes[0]
    serviceTime = serviceTimes[0]
    departureTime = arrivalTime + serviceTime

    qWaiting.append(0)
    sWaiting.append(departureTime)
    queueLength.append(len(queue))

    for i in range(1, n):
        arrivalTime += arrivalTimes[i]
        serviceTime = serviceTimes[i]
        while len(queue) > 0 and queue[0] <= arrivalTime:
            queue.pop(0)

        if arrivalTime < departureTime:
            queue.append(departureTime)
            qWaiting.append(departureTime - arrivalTime)
            sWaiting.append(departureTime - arrivalTime + serviceTime)
        else:
            qWaiting.append(0)
            sWaiting.append(serviceTime)

        queueLength.append(len(queue))

        departureTime = arrivalTime + serviceTime

    return np.mean(qWaiting), np.mean(sWaiting), np.mean(queueLength)

print(simulation(poissonArrival, exponentialService, 100000))
print(simulation(uniformArrival, exponentialService, 100000))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import interp1d
from scipy.stats import f_oneway
import random

def hypothesisTesting(x, y):
    result = f_oneway(x, y)
    if result[1] < 0.05:
        print("Significant Difference")
    else:
        print("No Significant Difference")

def newtonInterpolation(x, y, z):
    n = len(x)
    ddt = [[0 for i in range(n)] for j in range(n)]
    for i in range(n):
        ddt[i][0] = y[i]

    for i in range(1, n):
        for j in range(n - i):
            ddt[j][i] = ddt[j + 1][i - 1] - ddt[j][i - 1]

    u = (z - x[0]) / (x[1] - x[0])

    result = y[0]
    for i in range(1, n):
        temp = 1
        for j in range(i):
            temp *= (u - j) / (j + 1)
        result += ddt[0][i] * temp

    return result

def lagrangePolynomial(x, y, z):
    n = len(x)
    result = 0

    for i in range(n):
        temp = 1
        for j in range(n):
            if i != j:
                temp *= (z - x[j]) / (x[i] - x[j])
        result += y[i] * temp

    return result

def cubicSpline(x, y, z):
    splineFun = interp1d(x, y, kind = 'cubic')
    return splineFun(z)

with open("/content/drive/MyDrive/SEM-9/MM Lab/data.xls") as file:
    df = pd.read_csv(file)

df = df.loc[df['Year'] == 2020]
monthTemps = list(df.groupby(df['Month'])['AvgTemperature'].mean())
months = [(i + 1) for i in range(12)]

x = np.arange(1, 12, 0.1)
plt.scatter(months, monthTemps, label = 'Actual Points')
plt.plot(x, [newtonInterpolation(months, monthTemps, i) for i in x], label = 'Newton')
plt.plot(x, [lagrangePolynomial(months, monthTemps, i) for i in x], label = 'Lagrange')
plt.plot(x, [cubicSpline(months, monthTemps, i) for i in x], label = 'Spline')
plt.legend()
plt.show()

randomDays365 = random.sample(range(1, 366), 50)
randomDays365.sort()
randomDays12 = [1 + ((i - 1) / (365)) * 11  for i in randomDays365]
temps365 = [df['AvgTemperature'].iloc[i - 1] for i in randomDays365]

hypothesisTesting([newtonInterpolation(months, , z) for z in randomDays12], temps365)
hypothesisTesting([lagrangePolynomial(x, y, z) for z in randomDays12], temps365)
hypothesisTesting([cubicSpline(x, y, z) for z in randomDays12], temps365)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import interp1d
from scipy.stats import f_oneway
import random

def hypothesisTesting(x, y):
    result = f_oneway(x, y)
    if result[1] < 0.05:
        print("Significant Difference")
    else:
        print("No Significant Difference")

def newtonInterpolation(x, y, z):
    n = len(x)
    ddt = [[0 for i in range(n)] for j in range(n)]
    for i in range(n):
        ddt[i][0] = y[i]

    for i in range(1, n):
        for j in range(n - i):
            ddt[j][i] = ddt[j + 1][i - 1] - ddt[j][i - 1]

    u = (z - x[0]) / (x[1] - x[0])

    result = y[0]
    for i in range(1, n):
        temp = 1
        for j in range(i):
            temp *= (u - j) / (j + 1)
        result += ddt[0][i] * temp

    return result

def lagrangePolynomial(x, y, z):
    n = len(x)
    result = 0

    for i in range(n):
        temp = 1
        for j in range(n):
            if i != j:
                temp *= (z - x[j]) / (x[i] - x[j])
        result += y[i] * temp

    return result

def cubicSpline(x, y, z):
    splineFun = interp1d(x, y, kind = 'cubic')
    return splineFun(z)

with open("/content/drive/MyDrive/SEM-9/MM Lab/data.xls") as file:
    df = pd.read_csv(file)

df = df.loc[df['Year'] == 2020]
monthTemps = list(df.groupby(df['Month'])['AvgTemperature'].mean())
months = [(i + 1) for i in range(12)]

x = np.arange(1, 12, 0.1)
plt.scatter(months, monthTemps, label = 'Actual Points')
plt.plot(x, [newtonInterpolation(months, monthTemps, i) for i in x], label = 'Newton')
plt.plot(x, [lagrangePolynomial(months, monthTemps, i) for i in x], label = 'Lagrange')
plt.plot(x, [cubicSpline(months, monthTemps, i) for i in x], label = 'Spline')
plt.legend()
plt.show()

randomDays365 = random.sample(range(1, 366), 50)
randomDays365.sort()
randomDays12 = [1 + ((i - 1) / (365)) * 11  for i in randomDays365]
temps365 = [df['AvgTemperature'].iloc[i - 1] for i in randomDays365]

hypothesisTesting([newtonInterpolation(months, monthTemps, z) for z in randomDays12], temps365)
hypothesisTesting([lagrangePolynomial(months, monthTemps, z) for z in randomDays12], temps365)
hypothesisTesting([cubicSpline(months, monthTemps, z) for z in randomDays12], temps365)

import pandas as pd
import numpy as np
from scipy.stats import f_oneway, shapiro
from sklearn.metrics import r2_score

def hypothesisTesting(x, y):
    result = f_oneway(x, y)
    if result[1] < 0.05:
        print("Significant Difference")
    else:
        print("No Significant Difference")

def checkNormal(x):
    result = shapiro(x)
    if result[1] < 0.05:
        print("Not Normal")
    else:
        print("Normal")

with open("/content/drive/MyDrive/SEM-9/MM Lab/data.xls") as file:
    df = pd.read_csv(file)

Y = df['AvgTemperature'].to_list()
X = [(i + 1) for i in range(len(Y))]

xTrain = X[:7000]
yTrain = Y[:7000]
xTest = X[7000:]
yTest = Y[7000:]

model1 = np.poly1d(np.polyfit(xTrain, yTrain, 1))
model2 = np.poly1d(np.polyfit(xTrain, yTrain, 2))
model3 = np.poly1d(np.polyfit(xTrain, yTrain, 3))

yPred1 = model1(xTest)
yPred2 = model2(xTest)
yPred3 = model3(xTest)

hypothesisTesting(yPred1, yTest)
hypothesisTesting(yPred2, yTest)
hypothesisTesting(yPred3, yTest)

print(r2_score(yTest, yPred1))
print(r2_score(yTest, yPred2))
print(r2_score(yTest, yPred3))


plt.plot(xTest, yTest, label = 'Actual')
plt.plot(xTest, yPred1, label = 'Linear')
plt.plot(xTest, yPred2, label = 'Quadratic')
plt.plot(xTest, yPred3, label = 'Cubic')
plt.legend()
plt.show()

resid1 = yTest - yPred1
resid2 = yTest - yPred2
resid3 = yTest - yPred3

checkNormal(resid1)
checkNormal(resid2)
checkNormal(resid3)

residuals = [resid1, resid2, resid3]
means = [np.mean(resid) for resid in residuals]
stds = [np.std(resid) for resid in residuals]

X = [(i + 1) for i in range(100)]
plt.plot(X, model1(X) + np.random.normal(means[0], stds[0], 100), label = 'L-L')
plt.plot(X, model2(X) + np.random.normal(means[1], stds[1], 100), label = 'Q-L')
plt.plot(X, model3(X) + np.random.normal(means[2], stds[2], 100), label = 'C-L')
plt.legend()
plt.show()